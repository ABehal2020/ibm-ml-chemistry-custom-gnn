{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6da0058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\r\n",
      "#\r\n",
      "base                     /Users/adityabehal/opt/anaconda3\r\n",
      "aditya-ml                /Users/adityabehal/opt/anaconda3/envs/aditya-ml\r\n",
      "aixia                    /Users/adityabehal/opt/anaconda3/envs/aixia\r\n",
      "alpha0                   /Users/adityabehal/opt/anaconda3/envs/alpha0\r\n",
      "bondnet                  /Users/adityabehal/opt/anaconda3/envs/bondnet\r\n",
      "chipseq                  /Users/adityabehal/opt/anaconda3/envs/chipseq\r\n",
      "cs4150                   /Users/adityabehal/opt/anaconda3/envs/cs4150\r\n",
      "custom-gnn               /Users/adityabehal/opt/anaconda3/envs/custom-gnn\r\n",
      "gnn-tutorial             /Users/adityabehal/opt/anaconda3/envs/gnn-tutorial\r\n",
      "homl3                    /Users/adityabehal/opt/anaconda3/envs/homl3\r\n",
      "latest-custom-gnn     *  /Users/adityabehal/opt/anaconda3/envs/latest-custom-gnn\r\n",
      "rxnrep                   /Users/adityabehal/opt/anaconda3/envs/rxnrep\r\n",
      "search                   /Users/adityabehal/opt/anaconda3/envs/search\r\n",
      "si_env                   /Users/adityabehal/opt/anaconda3/envs/si_env\r\n",
      "socomp                   /Users/adityabehal/opt/anaconda3/envs/socomp\r\n",
      "spinal-stim              /Users/adityabehal/opt/anaconda3/envs/spinal-stim\r\n",
      "tf2                      /Users/adityabehal/opt/anaconda3/envs/tf2\r\n",
      "udemy-gnn                /Users/adityabehal/opt/anaconda3/envs/udemy-gnn\r\n",
      "wumpus                   /Users/adityabehal/opt/anaconda3/envs/wumpus\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120d798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 02:00:40.134909: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import rdkit, rdkit.Chem, rdkit.Chem.rdDepictor, rdkit.Chem.Draw\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import dense_to_sparse, add_self_loops, to_scipy_sparse_matrix\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from six.moves import urllib\n",
    "import deepchem as dc\n",
    "import random\n",
    "from dgl.nn import Set2Set\n",
    "import multiprocessing\n",
    "from torchvision.transforms import Compose, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a88af2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we want to train the model!\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to featurize datapoint 0, [Mo]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 0, [Al+3].[Al+3].[Mo].[Mo].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "Failed to featurize datapoint 0, [Ca+2].[Mg+2].[O-2].[O-2]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "Failed to featurize datapoint 0, [Mg+2]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 0, [Ca+2].[OH-].[OH-]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "Failed to featurize datapoint 0, [Ba+2].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[Fe+3].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "Failed to featurize datapoint 0, [K+].[K+].[O-2].[O-2].[O-2].[Ti+4]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "Failed to featurize datapoint 0, [Cd+2]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 0, [Cd+2].[OH-].[OH-]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "Failed to featurize datapoint 0, [OH-].[OH-].[Zn+2]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "Failed to featurize datapoint 0, [Re]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "[11:44:13] WARNING: not removing hydrogen atom without neighbors\n",
      "Failed to featurize datapoint 0, [F-].[F-].[H+].[NH4+]. Appending empty array\n",
      "Exception message: tuple index out of range\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 639\u001b[0m\n\u001b[1;32m    636\u001b[0m     runGraphNeuralNetwork()\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 639\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mprint(\"let's see what's coming out of our dataloader\")\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03mprint(\"xEdgeFError: \", xEdgeFError)\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 636\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 636\u001b[0m     \u001b[43mrunGraphNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 618\u001b[0m, in \u001b[0;36mrunGraphNeuralNetwork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    614\u001b[0m val_loss_epochs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwe want to train the model!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 618\u001b[0m test_data_loader, val_data_loader, train_data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "Cell \u001b[0;32mIn[2], line 117\u001b[0m, in \u001b[0;36mtrain_val_test_split\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_val_test_split\u001b[39m():\n\u001b[0;32m--> 117\u001b[0m     graph, sol \u001b[38;5;241m=\u001b[39m \u001b[43mfeaturize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m CustomDataset(graphAll\u001b[38;5;241m=\u001b[39mgraph, solAll\u001b[38;5;241m=\u001b[39msol, transform\u001b[38;5;241m=\u001b[39mCompose([ToTensor()]))\n\u001b[1;32m    121\u001b[0m     cores \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count() \u001b[38;5;66;03m# Count the number of cores in a computer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m, in \u001b[0;36mfeaturize_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# for i in range(100):\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(soldata)):\n\u001b[0;32m---> 80\u001b[0m     graphInstance \u001b[38;5;241m=\u001b[39m \u001b[43mgen_smiles2graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoldata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSMILES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graphInstance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_features\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graphInstance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graphInstance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_features\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     82\u001b[0m         graph\u001b[38;5;241m.\u001b[39mappend(graphInstance)\n",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m, in \u001b[0;36mgen_smiles2graph\u001b[0;34m(smiles)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Argument for the RD2NX function should be a valid SMILES sequence\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mreturns: the graph\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m featurizer \u001b[38;5;241m=\u001b[39m dc\u001b[38;5;241m.\u001b[39mfeat\u001b[38;5;241m.\u001b[39mMolGraphConvFeaturizer(use_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 66\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfeaturizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeaturize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/deepchem/deepchem/feat/base_classes.py:312\u001b[0m, in \u001b[0;36mMolecularFeaturizer.featurize\u001b[0;34m(self, datapoints, log_every_n, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    311\u001b[0m         kwargs_per_datapoint[key] \u001b[38;5;241m=\u001b[39m kwargs[key][i]\n\u001b[0;32m--> 312\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_featurize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_per_datapoint\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mol, Chem\u001b[38;5;241m.\u001b[39mrdchem\u001b[38;5;241m.\u001b[39mMol):\n",
      "File \u001b[0;32m~/Downloads/deepchem/deepchem/feat/molecule_featurizers/mol_graph_conv_featurizer.py:205\u001b[0m, in \u001b[0;36mMolGraphConvFeaturizer._featurize\u001b[0;34m(self, datapoint, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis class requires RDKit to be installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# construct atom (node) feature\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m h_bond_infos \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_hydrogen_bonding_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatapoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m atom_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    207\u001b[0m     [\n\u001b[1;32m    208\u001b[0m         _construct_atom_feature(atom, h_bond_infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_chirality,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# construct edge (bond) index\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/deepchem/deepchem/utils/molecule_feature_utils.py:242\u001b[0m, in \u001b[0;36mconstruct_hydrogen_bonding_info\u001b[0;34m(mol)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct hydrogen bonding infos about a molecule.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m  The `hydrogen_bonding_type` value is \"Acceptor\" or \"Donor\".\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m factory \u001b[38;5;241m=\u001b[39m _ChemicalFeaturesFactory\u001b[38;5;241m.\u001b[39mget_instance()\n\u001b[0;32m--> 242\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetFeaturesForMol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m hydrogen_bonding \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feats:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/custom-gnn/lib/python3.8/site-packages/rdkit/Chem/ChemicalFeatures.py:18\u001b[0m, in \u001b[0;36mMCFF_GetFeaturesForMol\u001b[0;34m(self, mol, includeOnly, confId)\u001b[0m\n\u001b[1;32m     16\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGetNumMolFeatures(mol, includeOnly\u001b[38;5;241m=\u001b[39mincludeOnly)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(count):\n\u001b[0;32m---> 18\u001b[0m   res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetMolFeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincludeOnly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincludeOnly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfId\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(res)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import rdkit, rdkit.Chem, rdkit.Chem.rdDepictor, rdkit.Chem.Draw\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import dense_to_sparse, add_self_loops, to_scipy_sparse_matrix\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from six.moves import urllib\n",
    "import deepchem as dc\n",
    "import random\n",
    "from dgl.nn import Set2Set\n",
    "import multiprocessing\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "def get_data():\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    sns.set_context(\"notebook\")\n",
    "    sns.set_style(\n",
    "        \"dark\",\n",
    "        {\n",
    "            \"xtick.bottom\": True,\n",
    "            \"ytick.left\": True,\n",
    "            \"xtick.color\": \"#666666\",\n",
    "            \"ytick.color\": \"#666666\",\n",
    "            \"axes.edgecolor\": \"#666666\",\n",
    "            \"axes.linewidth\": 0.8,\n",
    "            \"figure.dpi\": 300,\n",
    "        },\n",
    "    )\n",
    "    color_cycle = [\"#1BBC9B\", \"#F06060\", \"#5C4B51\", \"#F3B562\", \"#6e5687\"]\n",
    "    mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(color=color_cycle)\n",
    "\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    urllib.request.install_opener(opener)\n",
    "\n",
    "    # had to rehost because dataverse isn't reliable\n",
    "    soldata = pd.read_csv(\n",
    "        \"https://github.com/whitead/dmol-book/raw/main/data/curated-solubility-dataset.csv\"\n",
    "    )\n",
    "\n",
    "    return soldata\n",
    "\n",
    "def gen_smiles2graph(smiles):\n",
    "    \"\"\"Argument for the RD2NX function should be a valid SMILES sequence\n",
    "    returns: the graph\n",
    "    \"\"\"\n",
    "    featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "    out = featurizer.featurize(smiles)\n",
    "    return out[0]\n",
    "\n",
    "def featurize_data():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    soldata = get_data()\n",
    "\n",
    "    graph = []\n",
    "    sol = []\n",
    "\n",
    "    # for i in range(100):\n",
    "    for i in range(len(soldata)):\n",
    "        graphInstance = gen_smiles2graph(soldata.SMILES[i])\n",
    "        if hasattr(graphInstance, \"node_features\") and hasattr(graphInstance, \"edge_index\") and hasattr(graphInstance, \"edge_features\"):\n",
    "            graph.append(graphInstance)\n",
    "            sol.append(soldata.Solubility[i])\n",
    "\n",
    "    return graph, sol\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, graphAll, solAll, transform=None, target_transform=None):\n",
    "        self.graphInstances = graphAll\n",
    "        self.solInstances = solAll\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphInstances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graphInstance = self.graphInstances[idx]\n",
    "        solInstance = self.solInstances[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            graphInstanceNodeFeatures = self.transform(graphInstance.node_features)\n",
    "            graphInstanceEdgeIndex = self.transform(graphInstance.edge_index)\n",
    "            graphInstanceEdgeFeatures = self.transform(graphInstance.edge_features)\n",
    "\n",
    "            # B = torch.reshape(A, (A.shape[1], A.shape[2]))\n",
    "            graphInstanceNodeFeatures = torch.reshape(graphInstanceNodeFeatures, (graphInstanceNodeFeatures.shape[1], graphInstanceNodeFeatures.shape[2]))\n",
    "            graphInstanceEdgeIndex = torch.reshape(graphInstanceEdgeIndex, (graphInstanceEdgeIndex.shape[1], graphInstanceEdgeIndex.shape[2]))\n",
    "            graphInstanceEdgeFeatures = torch.reshape(graphInstanceEdgeFeatures, (graphInstanceEdgeFeatures.shape[1], graphInstanceEdgeFeatures.shape[2]))\n",
    "\n",
    "        if self.target_transform:\n",
    "            solInstance = self.target_transform(solInstance)\n",
    "\n",
    "        return graphInstanceNodeFeatures, graphInstanceEdgeIndex, graphInstanceEdgeFeatures, solInstance\n",
    "\n",
    "def train_val_test_split():\n",
    "    graph, sol = featurize_data()\n",
    "\n",
    "    dataset = CustomDataset(graphAll=graph, solAll=sol, transform=Compose([ToTensor()]))\n",
    "\n",
    "    cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "    print(\"cores: \", cores)\n",
    "\n",
    "    numWorkersToUse = 16 # 32 cores on workstation - let's try using 16\n",
    "\n",
    "    # batch_size=1 was original default - 100 was used in bondnet paper\n",
    "    # set num_workers=cores for best performance\n",
    "    dataloader = data.DataLoader(dataset, batch_size=1,\n",
    "                            shuffle=True, num_workers=numWorkersToUse)\n",
    "    # print(len(dataloader))\n",
    "    # print(type(dataloader))\n",
    "\n",
    "    test_data_size = int(0.1 * len(dataloader))\n",
    "    val_data_size = int(0.1 * len(dataloader))\n",
    "    train_data_size = len(dataloader) - 2 * int(0.1 * len(dataloader))\n",
    "\n",
    "    # test_data_size = 20\n",
    "    # val_data_size = 20\n",
    "    # train_data_size = 160\n",
    "\n",
    "    # test_data_size = 200\n",
    "    # val_data_size = 200\n",
    "    # train_data_size = len(dataloader) - 400\n",
    "\n",
    "    print(test_data_size)\n",
    "    print(val_data_size)\n",
    "    print(train_data_size)\n",
    "\n",
    "    test_data, val_data, train_data = data.random_split(dataloader, [test_data_size, val_data_size, train_data_size], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    # set num_workers=cores for best performance\n",
    "    test_data_loader = data.DataLoader(test_data, batch_size=1, shuffle=True, num_workers=numWorkersToUse)\n",
    "    val_data_loader = data.DataLoader(val_data, batch_size=1, shuffle=True, num_workers=numWorkersToUse)\n",
    "    train_data_loader = data.DataLoader(train_data, batch_size=1, shuffle=True, num_workers=numWorkersToUse)\n",
    "    # print(test_data)\n",
    "    # print(val_data)\n",
    "    # print(train_data)\n",
    "    # print(train_data.__getitem__(0))\n",
    "\n",
    "    return test_data_loader, val_data_loader, train_data_loader\n",
    "\n",
    "# create \"zeroth\" FCNN with 1 fully connected layer\n",
    "# condense node features from 30 to 24\n",
    "# dilate edge features from 11 to 24\n",
    "# see Table S4 BDNCM input feature embedding size 24: https://www.rsc.org/suppdata/d0/sc/d0sc05251e/d0sc05251e1.pdf\n",
    "class InitialEmbedding(torch.nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.fc_initial_embedding = nn.Linear(c_in, c_out)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        features = self.fc_initial_embedding(features)\n",
    "        features = F.relu(features)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# neural network with two fully connected layers\n",
    "class FCNN(torch.nn.Module):\n",
    "    # c_in1 = 24, c_out1 = 256, c_out2 = 24\n",
    "    def __init__(self, c_in1, c_out1, c_out2):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.fc1 = nn.Linear(c_in1, c_out1)\n",
    "        torch.manual_seed(0)\n",
    "        self.fc2 = nn.Linear(c_out1, c_out2)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # print(\"input 2 layer FCNN features: \", features)\n",
    "        features = self.fc1(features)\n",
    "        # print(\"after 1 linear layer updated features: \", features)\n",
    "        features = F.relu(features)\n",
    "        # print(\"after ReLu updated features: \", features)\n",
    "        features = self.fc2(features)\n",
    "        \n",
    "        # print(\"after 2 linear layers updated features: \", features)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# implementation of equation 5 in bondnet paper \n",
    "# https://pubs.rsc.org/en/content/articlepdf/2021/sc/d0sc05251e\n",
    "class NodeFeatures(torch.nn.Module):\n",
    "    # c_in1 = 24, c_out1 = 24, c_out2 = 24\n",
    "    def __init__(self, c_in1, c_out1, c_out2):\n",
    "        super().__init__()\n",
    "        # self.fc_initial_embedding = InitialEmbedding(c_in=30, c_out=24)\n",
    "        self.FCNN_one = FCNN(c_in1=c_in1, c_out1=c_out1, c_out2=c_out2)\n",
    "        self.FCNN_two = FCNN(c_in1=c_in1, c_out1=c_out1, c_out2=c_out2)\n",
    "        \n",
    "    def forward(self, node_features, edge_index, edge_features):\n",
    "        sigmoidFunction = torch.nn.Sigmoid()\n",
    "        \n",
    "        original_node_features = node_features.detach().clone()\n",
    "        \n",
    "        epsilon = 1e-7\n",
    "\n",
    "        for i in range(node_features.shape[1]):\n",
    "            # DOUBLE CHECK WITH DAS\n",
    "            # intermediate_node_feature = self.FCNN_one(node_features[i].T)\n",
    "            intermediate_node_feature = self.FCNN_one(original_node_features[0][i])\n",
    "            \n",
    "            other_nodes_indices = []\n",
    "            other_edges_indices = []\n",
    "            \n",
    "            other_edges_numerators = []\n",
    "            other_edges_denominator = epsilon\n",
    "            \n",
    "            '''\n",
    "            print(\"node_features[i].T: \", node_features[i].T)\n",
    "            print(\"node_features[i].T.size: \", node_features[i].T.size())\n",
    "            \n",
    "            print(\"intermediate_node_feature: \", intermediate_node_feature)\n",
    "            print(\"intermediate_node_feature.size: \", intermediate_node_feature.size())\n",
    "            '''\n",
    "            \n",
    "            for j in range(edge_index.shape[1]):\n",
    "                if edge_index[0][0][j] == i:\n",
    "                    other_nodes_indices.append(int(edge_index[0][1][j]))\n",
    "                    other_edges_indices.append(j)\n",
    "                if edge_index[0][1][j] == i:\n",
    "                    other_nodes_indices.append(int(edge_index[0][0][j]))\n",
    "                    other_edges_indices.append(j)\n",
    "            \n",
    "            '''\n",
    "            print(\"current node index: \", i)\n",
    "            print(\"other_nodes_indices: \", other_nodes_indices)\n",
    "            print(\"other_edges_indices: \", other_edges_indices)\n",
    "            '''\n",
    "            \n",
    "            for other_edge_index in other_edges_indices:\n",
    "                # print(\"SIGMOID ALERT TEST TEST TEST: \", sigmoidFunction(edge_features[other_edge_index]))\n",
    "                other_edges_numerators.append(sigmoidFunction(edge_features[0][other_edge_index]))\n",
    "                other_edges_denominator += sigmoidFunction(edge_features[0][other_edge_index])\n",
    "                \n",
    "            # print(\"other_edges_numerators: \", other_edges_numerators)\n",
    "            # print(\"other_edges_denominator: \", other_edges_denominator)\n",
    "            \n",
    "            for other_edge_numerator, other_node_index in zip(other_edges_numerators, other_nodes_indices):\n",
    "                edge_hat = other_edge_numerator/other_edges_denominator\n",
    "                # DOUBLE CHECK WITH DAS\n",
    "                # other_node_updated = self.FCNN_two(node_features[other_node_index].T) \n",
    "                other_node_updated = self.FCNN_two(original_node_features[0][other_node_index].T)\n",
    "                intermediate_node_feature += edge_hat * other_node_updated\n",
    "                \n",
    "                # print(\"edge_hat: \", edge_hat)\n",
    "                '''\n",
    "                print(\"node_features[other_node_index].T: \", node_features[other_node_index].T)\n",
    "                print(\"node_features[other_node_index].T.size: \", node_features[other_node_index].T.size())\n",
    "                print(\"other_node_updated: \", other_node_updated)\n",
    "                print(\"other_node_updated.size: \", other_node_updated.size())\n",
    "                '''\n",
    "                \n",
    "            # print(\"intermediate_node_feature: \", intermediate_node_feature)\n",
    "            # print(\"intermediate_node_feature.size: \", intermediate_node_feature.size())\n",
    "                \n",
    "            '''\n",
    "            print(\"reLuOutput: \", F.relu(intermediate_node_feature))\n",
    "            print(\"reLuOutput.size: \", F.relu(intermediate_node_feature).size())\n",
    "            print(\"original_node_features[i].T\", original_node_features[i].T)\n",
    "            print(\"original_node_features[i].T.size\", original_node_features[i].T.size())\n",
    "            print(\"calculated updated node_features[i]: \", (original_node_features[i].T + F.relu(intermediate_node_feature)).T)\n",
    "            print(\"calculated updated node_features[i].size(): \", (original_node_features[i].T + F.relu(intermediate_node_feature)).T.size())\n",
    "            '''\n",
    "            \n",
    "            # UPDATE TO INCLUDE THIS AT SOME POINT\n",
    "            # intermediate_node_feature --> batch normalization --> drop out --> then ReLu\n",
    "            # should I use batch norm 1D and what should my feature size be at this point?\n",
    "            '''\n",
    "            batchNorm1dLayer = nn.BatchNorm1d(intermediate_node_feature.size(dim=0))\n",
    "            dropoutLayer = nn.Dropout(p=0.1)\n",
    "            \n",
    "            intermediate_node_feature = batchNorm1dLayer(torch.reshape(intermediate_node_feature, (1, intermediate_node_feature.size(dim=0))))\n",
    "            intermediate_node_feature = torch.reshape(intermediate_node_feature, (-1,))\n",
    "            intermediate_node_feature = dropoutLayer(intermediate_node_feature)\n",
    "            '''\n",
    "            \n",
    "            instanceNorm1dLayer = nn.InstanceNorm1d(intermediate_node_feature.size(dim=0))\n",
    "            dropoutLayer = nn.Dropout(p=0.1)\n",
    "            \n",
    "            intermediate_node_feature = instanceNorm1dLayer(torch.reshape(intermediate_node_feature, (1, intermediate_node_feature.size(dim=0))))\n",
    "            intermediate_node_feature = torch.reshape(intermediate_node_feature, (-1,))\n",
    "            intermediate_node_feature = dropoutLayer(intermediate_node_feature)\n",
    "            \n",
    "            # node_features[i] = F.relu(intermediate_node_feature).T\n",
    "            node_features[0][i] = (original_node_features[0][i].T + F.relu(intermediate_node_feature)).T\n",
    "            \n",
    "            # print(\"actually updated node_features[i]: \", node_features[0][i])\n",
    "            # print(\"actually updated node_features[i].size(): \", node_features[0][i].size())\n",
    "            # print(\"********** NODE UPDATED SUCCESSFULLY ****************\")\n",
    "            \n",
    "        return node_features\n",
    "        \n",
    "# implementation of equation 4 in bondnet paper\n",
    "# https://pubs.rsc.org/en/content/articlepdf/2021/sc/d0sc05251e\n",
    "class EdgeFeatures(torch.nn.Module):\n",
    "    # c_in1 = 24, c_out1 = 24, c_out2 = 24\n",
    "    def __init__(self, c_in1, c_out1, c_out2):\n",
    "        super().__init__()\n",
    "        # self.fc_initial_embedding = InitialEmbedding(c_in=11, c_out=24)\n",
    "        self.FCNN_one = FCNN(c_in1=c_in1, c_out1=c_out1, c_out2=c_out2)\n",
    "        self.FCNN_two = FCNN(c_in1=c_in1, c_out1=c_out1, c_out2=c_out2)\n",
    "        \n",
    "    def forward(self, node_features, edge_index, edge_features):\n",
    "        original_edge_features = edge_features.detach().clone()\n",
    "        \n",
    "        for i in range(edge_index.shape[1]):\n",
    "            # summing node features involved in the given edge and transforming them\n",
    "            firstNodeIndex = int(edge_index[0][0][i])\n",
    "            secondNodeIndex = int(edge_index[0][1][i])\n",
    "            node_features_sum = node_features[0][firstNodeIndex] + node_features[0][secondNodeIndex]\n",
    "            intermediate_node_features = self.FCNN_one(node_features_sum.T)\n",
    "\n",
    "            '''\n",
    "            print(\"firstNodeIndex: \", firstNodeIndex)\n",
    "            print(\"secondNodeIndex: \", secondNodeIndex)\n",
    "            print(\"node_features[firstNodeIndex]: \", node_features[0][firstNodeIndex])\n",
    "            print(\"node_features[secondNodeIndex]: \", node_features[0][secondNodeIndex])\n",
    "            print(\"node_features_sum: \", node_features_sum)\n",
    "            print(\"node_features_sum.size: \", node_features_sum.size())\n",
    "            print(\"node_features_sum.T: \", node_features_sum.T)\n",
    "            print(\"node_features_sum.T.size: \", node_features_sum.T.size())\n",
    "            print(\"intermediate_node_features: \", intermediate_node_features)\n",
    "            print(\"intermediate_node_features.size: \", intermediate_node_features.size())\n",
    "            '''\n",
    "            \n",
    "            # transforming the features of the given edge \n",
    "            intermediate_edge_feature = self.FCNN_two(edge_features[0][i].T)\n",
    "\n",
    "            '''\n",
    "            print(\"edge_features index: \", i)\n",
    "            print(\"edge_features: \", edge_features[0][i])\n",
    "            print(\"edge_features.size: \", edge_features[0][i].size())\n",
    "            print(\"edge_features.T: \", edge_features[0][i].T)\n",
    "            print(\"edge_features.T.size(): \", edge_features[0][i].T.size())\n",
    "            print(\"intermediate_edge_feature: \", intermediate_edge_feature)\n",
    "            print(\"intermediate_edge_feature.size: \", intermediate_edge_feature.size())\n",
    "            print(\"intermediate_edge_feature.size dim 0: \", intermediate_edge_feature.size(dim=0))\n",
    "            '''\n",
    "\n",
    "            # merging node features with features of the given edge\n",
    "            \n",
    "            # UPDATE TO INCLUDE THIS AT SOME POINT\n",
    "            # intermediate_node_features + intermediate_edge_feature --> batch normalization --> drop out --> then ReLu\n",
    "            \n",
    "            intermediate_features_relu_input = intermediate_node_features + intermediate_edge_feature\n",
    "            \n",
    "            instanceNorm1dLayer = nn.InstanceNorm1d(intermediate_features_relu_input.size(dim=0))\n",
    "            dropoutLayer = nn.Dropout(p=0.1)\n",
    "            \n",
    "            intermediate_features_relu_input = instanceNorm1dLayer(torch.reshape(intermediate_features_relu_input, (1, intermediate_features_relu_input.size(dim=0))))\n",
    "            intermediate_features_relu_input = torch.reshape(intermediate_features_relu_input, (-1,))                                                              \n",
    "            intermediate_features_relu_input = dropoutLayer(intermediate_features_relu_input)\n",
    "            \n",
    "            intermediate_features = F.relu(intermediate_features_relu_input)\n",
    "\n",
    "            '''\n",
    "            print(\"intermediate_features: \", intermediate_features)\n",
    "            print(\"intermediate_features.size: \", intermediate_features.size())\n",
    "            print(\"original_edge_features[i].T: \", original_edge_features[0][i].T)\n",
    "            print(\"calculated updated edge_features[i]: \", (original_edge_features[0][i].T + intermediate_features).T)\n",
    "            '''\n",
    "\n",
    "            # updating edge features\n",
    "            edge_features[0][i] = (original_edge_features[0][i].T + intermediate_features).T\n",
    "\n",
    "            '''\n",
    "            print(\"actually updated edge_features[i]: \", edge_features[0][i])\n",
    "            print(\"********** EDGE UPDATED SUCCESSFULLY ****************\")\n",
    "            '''\n",
    "            \n",
    "        return edge_features\n",
    "    \n",
    "class Graph2Graph(torch.nn.Module):\n",
    "    def __init__(self, c_in1, c_out1, c_out2):\n",
    "        super().__init__()\n",
    "        self.NodeFeaturesConvolution = NodeFeatures(c_in1, c_out1, c_out2)\n",
    "        self.EdgeFeaturesConvolution = EdgeFeatures(c_in1, c_out1, c_out2)\n",
    "        \n",
    "    def forward(self, node_features, edge_index, edge_features):\n",
    "        # print(\"node_features_shape: \", node_features.shape)\n",
    "        # print(\"edge_features_shape: \", edge_features.shape)\n",
    "        node_features = self.NodeFeaturesConvolution(node_features, edge_index, edge_features)\n",
    "        edge_features = self.EdgeFeaturesConvolution(node_features, edge_index, edge_features)\n",
    "        \n",
    "        return node_features, edge_features\n",
    "    \n",
    "class Features_Set2Set():\n",
    "    def __init__(self, initial_dim_out):\n",
    "        self.node_s2s = Set2Set(initial_dim_out, 6, 3)\n",
    "        self.edge_s2s = Set2Set(initial_dim_out, 6, 3)\n",
    "    \n",
    "    def transform_then_concat(self, node_features, edge_index, edge_features):\n",
    "        node_features = torch.reshape(node_features, (node_features.shape[1], node_features.shape[2]))\n",
    "        edge_index = torch.reshape(edge_index, (edge_index.shape[1], edge_index.shape[2]))\n",
    "        edge_features = torch.reshape(edge_features, (edge_features.shape[1], edge_features.shape[2]))\n",
    "\n",
    "        node_features = node_features.detach().numpy()\n",
    "        edge_index = edge_index.detach().numpy().astype(int)\n",
    "        edge_features = edge_features.detach().numpy()\n",
    "\n",
    "        # edge_index = edge_index[:, ::2]\n",
    "        # edge_features = edge_features[::2, :]\n",
    "\n",
    "        deepchem_graph_nodes = dc.feat.GraphData(node_features, edge_index, edge_features)\n",
    "        dgl_graph_nodes = deepchem_graph_nodes.to_dgl_graph()\n",
    "\n",
    "        # CHECK WITH DAS - ask what is 1 x 48 being outputted with both node and edge features from set2set - note: this is correct\n",
    "        node_features_transformed = self.node_s2s(dgl_graph_nodes, torch.from_numpy(node_features))\n",
    "\n",
    "        # CHECK WITH DAS to make sure this is an ok way to use set2set on edges\n",
    "        while (node_features.shape[0] < edge_features.shape[0]):\n",
    "            node_features = np.append(node_features, np.array([np.zeros(node_features.shape[1])]), axis=0)\n",
    "\n",
    "        while (node_features.shape[0] > edge_features.shape[0]):\n",
    "            edge_features = np.append(edge_features, np.array([np.zeros(edge_features.shape[1])]), axis=0)\n",
    "            try:\n",
    "                # edge_index = np.append(edge_index, np.array([[edge_index.shape[1], edge_index.shape[1]]]), axis=1)\n",
    "                edge_index = np.column_stack((edge_index, np.array([edge_index.shape[1], edge_index.shape[1]])))\n",
    "            except:\n",
    "                print(\"let's investigate further!\")\n",
    "\n",
    "        if (edge_index.shape[1] % 2) == 1:\n",
    "            node_features = np.append(node_features, np.array([np.zeros(node_features.shape[1])]), axis=0)\n",
    "            edge_index = np.column_stack((edge_index, np.array([edge_index.shape[1], edge_index.shape[1]])))\n",
    "            edge_features = np.append(edge_features, np.array([np.zeros(edge_features.shape[1])]), axis=0)\n",
    "\n",
    "        deepchem_graph_edges = dc.feat.GraphData(edge_features, edge_index, node_features)\n",
    "        dgl_graph_edges = deepchem_graph_edges.to_dgl_graph()\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            edge_features_transformed = self.edge_s2s(dgl_graph_edges, torch.from_numpy(edge_features))\n",
    "        except Exception as e:\n",
    "            print(\"node features data type: \", node_features.dtype)\n",
    "            print(\"edge index data type: \", edge_index.dtype)\n",
    "            print(\"edge features data type: \", edge_features.dtype)\n",
    "            print(e)\n",
    "            print(\"ok interesting!\")\n",
    "        '''\n",
    "\n",
    "        edge_features_transformed = self.edge_s2s(dgl_graph_edges, torch.from_numpy(edge_features).to(torch.float32))\n",
    "\n",
    "        # intermediate_node_feature = torch.reshape(intermediate_node_feature, (-1,))\n",
    "        node_features_transformed = torch.reshape(node_features_transformed, (-1,))\n",
    "        edge_features_transformed = torch.reshape(edge_features_transformed, (-1,))\n",
    "        \n",
    "        return torch.cat((node_features_transformed, edge_features_transformed))\n",
    "\n",
    "class Graph2Property(torch.nn.Module):\n",
    "    # c_in1 = 24, c_out1 = 256, c_out2 = 128, c_out3 = 64, c_out4 = 1\n",
    "    def __init__(self, c_in1, c_out1, c_out2, c_out3, c_out4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(c_in1, c_out1)\n",
    "        self.fc2 = nn.Linear(c_out1, c_out2)\n",
    "        self.fc3 = nn.Linear(c_out2, c_out3)\n",
    "        self.fc4 = nn.Linear(c_out3, c_out4)\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.fc1(features)\n",
    "        features = F.relu(features)\n",
    "        features = self.fc2(features)\n",
    "        features = F.relu(features)\n",
    "        features = self.fc3(features)\n",
    "        features = F.relu(features)\n",
    "        features = self.fc4(features)\n",
    "\n",
    "        return features\n",
    "        \n",
    "class GraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, nodes_initial_dim_in=30, edges_initial_dim_in=11, initial_dim_out=24, g2g_input_dim=96, g2g_hidden_dim=256, g2p_dim_1=256, g2p_dim_2=128, g2p_dim_3=64):\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "        self.nodes_initial_embedding = InitialEmbedding(nodes_initial_dim_in, initial_dim_out)\n",
    "        self.edges_initial_embedding = InitialEmbedding(edges_initial_dim_in, initial_dim_out)\n",
    "        self.g2g_module = Graph2Graph(initial_dim_out, g2g_hidden_dim, initial_dim_out)\n",
    "        self.features_set2set = Features_Set2Set(initial_dim_out)\n",
    "        self.g2p_module = Graph2Property(g2g_input_dim, g2p_dim_1, g2p_dim_2, g2p_dim_3, 1)\n",
    "        \n",
    "    def forward(self, X1, X2, X3, g2g_num=4):\n",
    "        '''\n",
    "        node_features = graph_instance.node_features \n",
    "        edge_index = graph_instance.edge_index\n",
    "        edge_features = graph_instance.edge_features\n",
    "        '''\n",
    "\n",
    "        node_features = X1\n",
    "        edge_index = X2\n",
    "        edge_features = X3\n",
    "\n",
    "        '''\n",
    "        print(\"node_features: \", node_features.shape)\n",
    "        print(\"edge_index: \", edge_index.shape)\n",
    "        print(\"edge_features: \", edge_features)\n",
    "        '''\n",
    "        \n",
    "        node_features_updated = self.nodes_initial_embedding(node_features)\n",
    "        edge_features_updated = self.edges_initial_embedding(edge_features)\n",
    "        \n",
    "        for i in range(g2g_num):\n",
    "            node_features_updated, edge_features_updated = self.g2g_module(node_features_updated, edge_index, edge_features_updated)\n",
    "            \n",
    "        features_concatenated = self.features_set2set.transform_then_concat(node_features_updated, edge_index, edge_features_updated)\n",
    "        \n",
    "        predicted_value = self.g2p_module(features_concatenated)\n",
    "        \n",
    "        return predicted_value\n",
    "\n",
    "def train_loop(dataloader, dataloader2, model, loss_fn, optimizer):\n",
    "    # print(\"before size\")\n",
    "    size = len(dataloader.dataset)\n",
    "    # print(\"after size\")\n",
    "    # print(size)\n",
    "    loss_batch = []\n",
    "    for batch, (X1, X2, X3, y) in enumerate(dataloader.dataset):\n",
    "        pred = model(X1.float(), X2.float(), X3.float())\n",
    "        yReshaped = torch.Tensor([y])\n",
    "        # print(yReshaped.shape)\n",
    "        # print(\"Prediction: %s, Actual value %s\", pred, yReshaped)\n",
    "        loss = loss_fn(pred, yReshaped)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_batch.append(loss.item())\n",
    "\n",
    "    loss_epoch = np.average(loss_batch)\n",
    "\n",
    "    print(\"Training loss: \", loss_epoch)\n",
    "\n",
    "    val_loss_batch = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X1, X2, X3, y) in enumerate(dataloader2.dataset):\n",
    "            pred = model(X1.float(), X2.float(), X3.float())\n",
    "            yReshaped = torch.Tensor([y])\n",
    "            loss = loss_fn(pred, yReshaped)\n",
    "      \n",
    "            val_loss_batch.append(loss.item())\n",
    "    \n",
    "    val_loss_epoch = np.average(val_loss_batch)\n",
    "\n",
    "    print(\"Validation loss: \", val_loss_epoch)\n",
    "\n",
    "    return loss_epoch, val_loss_epoch\n",
    "\n",
    "'''\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader.dataset:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "'''\n",
    "\n",
    "def plotLearningCurves(train_loss, val_loss):\n",
    "    print(len(train_loss))\n",
    "    print(len(val_loss))\n",
    "\n",
    "    print(\"train_loss: \", train_loss)\n",
    "    print(\"val_loss: \", val_loss)\n",
    "\n",
    "    train_loss = torch.Tensor(train_loss)\n",
    "    val_loss = torch.Tensor(val_loss)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xticks(range(1, len(train_loss) + 1))\n",
    "    plt.plot([i + 1 for i in range(len(train_loss))], val_loss, label=\"val\")\n",
    "    plt.plot([i + 1 for i in range(len(train_loss))], train_loss, label=\"train\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def runGraphNeuralNetwork():\n",
    "    model = GraphNeuralNetwork()\n",
    "    model = model.float()\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    loss_epochs = []\n",
    "    val_loss_epochs = []\n",
    "\n",
    "    print(\"we want to train the model!\")\n",
    "\n",
    "    test_data_loader, val_data_loader, train_data_loader = train_val_test_split()\n",
    "\n",
    "    epochs = 20\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "        # we need to check to see if train_data and val_data is being shuffled before each epoch along with playing around with different initializations (and can do multiple reruns)\n",
    "        # we can also try SGD for a few epochs (5) before doing Adam or maybe try SGD for all 20 epochs\n",
    "        # we can run several jupyter notebooks in parallel\n",
    "        train_loss_epoch_value, val_loss_epoch_value = train_loop(train_data_loader.dataset, val_data_loader.dataset,\n",
    "                                                                  model, loss_fn, optimizer)\n",
    "        train_loss.append(train_loss_epoch_value)\n",
    "        val_loss.append(val_loss_epoch_value)\n",
    "        # test_loop(test_data, model, loss_fn)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    plotLearningCurves(train_loss, val_loss)\n",
    "\n",
    "def main():\n",
    "    runGraphNeuralNetwork()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "'''\n",
    "print(\"let's see what's coming out of our dataloader\")\n",
    "\n",
    "xNodeFError = 0\n",
    "xEdgeIError = 0\n",
    "xEdgeFError = 0\n",
    "\n",
    "for batch, (X, y) in enumerate(dataloader.dataset):\n",
    "    if not hasattr(X, \"node_features\"):\n",
    "        xNodeFError += 1\n",
    "    if not hasattr(X, \"edge_index\"):\n",
    "        xEdgeIError += 1\n",
    "    if not hasattr(X, \"edge_features\"):\n",
    "        xEdgeFError += 1\n",
    "\n",
    "print(\"xNodeError: \", xNodeFError)\n",
    "print(\"xEdgeIError: \", xEdgeIError)\n",
    "print(\"xEdgeFError: \", xEdgeFError)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
